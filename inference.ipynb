{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU codecarbon\n",
    "!pip install -qU py-cpuinfo\n",
    "!pip install -qU onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import onnxruntime as ort\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel,\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "import torch.nn as nn\n",
    "from codecarbon import EmissionsTracker\n",
    "import psutil\n",
    "import cpuinfo\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_LEN = 150\n",
    "MODEL_TYPES = [\"bert-base-uncased\", \"distilbert-base-uncased\", \"google/tinybert-6l-768d\"]\n",
    "MODEL_NAMES = [\"BERT\", \"DistilBERT\", \"TinyBERT\"]\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Path configurations\n",
    "ONNX_DIR = \"onnx_checkpoints\"\n",
    "PT_DIR = \"pt_checkpoints\"\n",
    "RESULTS_DIR = \"benchmark_results\"\n",
    "CPP_EXECUTABLE = \"onnx_inference_cpp\"  # Will be created\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example data (we'll create synthetic test data for benchmarking)\n",
    "def generate_test_data(n_samples=100, max_length=150):\n",
    "    \"\"\"Generate synthetic test data for benchmarking.\"\"\"\n",
    "    # Generate random texts of varying lengths\n",
    "    texts = []\n",
    "    for _ in range(n_samples):\n",
    "        # Random length between 50 and max_length words\n",
    "        length = np.random.randint(50, max_length)\n",
    "        # Generate random \"words\" (we don't need actual words for benchmarking)\n",
    "        text = \" \".join([f\"word{i}\" for i in range(length)])\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Model class for PyTorch inference\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        if 'distilbert' in model_name:\n",
    "            self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        elif 'tinybert' in model_name:\n",
    "            self.bert = AutoModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if hasattr(self.bert, 'distilbert'):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        else:\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "        \n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Inference utilities\n",
    "def get_tokenizer(model_type):\n",
    "    \"\"\"Get the appropriate tokenizer for the model type.\"\"\"\n",
    "    if 'distilbert' in model_type:\n",
    "        return DistilBertTokenizer.from_pretrained(model_type)\n",
    "    elif 'tinybert' in model_type:\n",
    "        return AutoTokenizer.from_pretrained(model_type)\n",
    "    else:\n",
    "        return BertTokenizer.from_pretrained(model_type)\n",
    "\n",
    "def load_pytorch_model(model_path, model_type, num_labels=100):\n",
    "    \"\"\"Load a PyTorch model from a checkpoint.\"\"\"\n",
    "    model = BERTClassifier(model_type, num_labels)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Inference functions\n",
    "def pytorch_inference(model, tokenizer, texts, batch_size=1):\n",
    "    \"\"\"Run inference using PyTorch.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask']\n",
    "            )\n",
    "            \n",
    "        # Convert to predictions (we don't need actual labels for benchmarking)\n",
    "        predictions = torch.sigmoid(outputs).numpy() > 0.5\n",
    "        results.extend(predictions.tolist())\n",
    "    \n",
    "    return results\n",
    "\n",
    "def onnx_inference(onnx_path, tokenizer, texts, batch_size=1):\n",
    "    \"\"\"Run inference using ONNX Runtime.\"\"\"\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get input names\n",
    "        input_names = [input.name for input in session.get_inputs()]\n",
    "        \n",
    "        # Create the input feed\n",
    "        ort_inputs = {\n",
    "            input_names[0]: inputs['input_ids'].numpy(),\n",
    "            input_names[1]: inputs['attention_mask'].numpy()\n",
    "        }\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run(None, ort_inputs)\n",
    "        \n",
    "        # Process outputs\n",
    "        logits = outputs[0]\n",
    "        predictions = (1 / (1 + np.exp(-logits))) > 0.5\n",
    "        results.extend(predictions.tolist())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create C++ code for ONNX inference\n",
    "def create_cpp_inference_code():\n",
    "    \"\"\"Create a C++ file for ONNX inference.\"\"\"\n",
    "    cpp_code = \"\"\"\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <vector>\n",
    "#include <string>\n",
    "#include <chrono>\n",
    "#include <onnxruntime_cxx_api.h>\n",
    "\n",
    "struct InferenceResult {\n",
    "    std::vector<std::vector<bool>> predictions;\n",
    "    double inference_time_ms;\n",
    "};\n",
    "\n",
    "InferenceResult run_inference(const std::string& model_path, \n",
    "                             const std::vector<std::vector<int64_t>>& input_ids,\n",
    "                             const std::vector<std::vector<int64_t>>& attention_mask) {\n",
    "    // Initialize ONNX Runtime\n",
    "    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"ONNXInference\");\n",
    "    Ort::SessionOptions session_options;\n",
    "    session_options.SetIntraOpNumThreads(1);\n",
    "    session_options.SetInterOpNumThreads(1);\n",
    "    session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);\n",
    "\n",
    "    // Create session\n",
    "    Ort::Session session(env, model_path.c_str(), session_options);\n",
    "\n",
    "    // Get input and output names\n",
    "    Ort::AllocatorWithDefaultOptions allocator;\n",
    "    std::vector<const char*> input_names = {\"input_ids\", \"attention_mask\"};\n",
    "    std::vector<const char*> output_names = {\"output\"};\n",
    "\n",
    "    // Prepare input tensors\n",
    "    std::vector<int64_t> input_dims = {static_cast<int64_t>(input_ids.size()), static_cast<int64_t>(input_ids[0].size())};\n",
    "    \n",
    "    std::vector<int64_t> input_ids_flattened;\n",
    "    std::vector<int64_t> attention_mask_flattened;\n",
    "    \n",
    "    for (const auto& ids : input_ids) {\n",
    "        input_ids_flattened.insert(input_ids_flattened.end(), ids.begin(), ids.end());\n",
    "    }\n",
    "    \n",
    "    for (const auto& mask : attention_mask) {\n",
    "        attention_mask_flattened.insert(attention_mask_flattened.end(), mask.begin(), mask.end());\n",
    "    }\n",
    "\n",
    "    Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\n",
    "    \n",
    "    std::vector<Ort::Value> input_tensors;\n",
    "    input_tensors.push_back(Ort::Value::CreateTensor<int64_t>(\n",
    "        memory_info, input_ids_flattened.data(), input_ids_flattened.size(), \n",
    "        input_dims.data(), input_dims.size()));\n",
    "    \n",
    "    input_tensors.push_back(Ort::Value::CreateTensor<int64_t>(\n",
    "        memory_info, attention_mask_flattened.data(), attention_mask_flattened.size(), \n",
    "        input_dims.data(), input_dims.size()));\n",
    "\n",
    "    // Run inference\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    std::vector<Ort::Value> output_tensors = session.Run(\n",
    "        Ort::RunOptions{nullptr}, \n",
    "        input_names.data(), \n",
    "        input_tensors.data(), \n",
    "        input_tensors.size(), \n",
    "        output_names.data(), \n",
    "        output_names.size());\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    // Calculate inference time\n",
    "    std::chrono::duration<double, std::milli> inference_time = end - start;\n",
    "\n",
    "    // Process outputs\n",
    "    float* output_data = output_tensors[0].GetTensorMutableData<float>();\n",
    "    \n",
    "    int64_t batch_size = input_dims[0];\n",
    "    int64_t num_labels = output_tensors[0].GetTensorTypeAndShapeInfo().GetShape()[1];\n",
    "    \n",
    "    // Convert logits to predictions\n",
    "    std::vector<std::vector<bool>> predictions(batch_size, std::vector<bool>(num_labels));\n",
    "    for (int64_t i = 0; i < batch_size; i++) {\n",
    "        for (int64_t j = 0; j < num_labels; j++) {\n",
    "            float logit = output_data[i * num_labels + j];\n",
    "            float sigmoid = 1.0f / (1.0f + exp(-logit));\n",
    "            predictions[i][j] = sigmoid > 0.5f;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {predictions, inference_time.count()};\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "    if (argc < 2) {\n",
    "        std::cerr << \"Usage: \" << argv[0] << \" <model_path> <input_file> <output_file>\" << std::endl;\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    std::string model_path = argv[1];\n",
    "    std::string input_file = argv[2];\n",
    "    std::string output_file = argv[3];\n",
    "    \n",
    "    // Read input data from file\n",
    "    std::ifstream input(input_file);\n",
    "    if (!input.is_open()) {\n",
    "        std::cerr << \"Could not open input file: \" << input_file << std::endl;\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    std::vector<std::vector<int64_t>> input_ids;\n",
    "    std::vector<std::vector<int64_t>> attention_mask;\n",
    "    \n",
    "    size_t batch_size, seq_len;\n",
    "    input >> batch_size >> seq_len;\n",
    "    \n",
    "    for (size_t i = 0; i < batch_size; i++) {\n",
    "        std::vector<int64_t> ids(seq_len);\n",
    "        for (size_t j = 0; j < seq_len; j++) {\n",
    "            input >> ids[j];\n",
    "        }\n",
    "        input_ids.push_back(ids);\n",
    "    }\n",
    "    \n",
    "    for (size_t i = 0; i < batch_size; i++) {\n",
    "        std::vector<int64_t> mask(seq_len);\n",
    "        for (size_t j = 0; j < seq_len; j++) {\n",
    "            input >> mask[j];\n",
    "        }\n",
    "        attention_mask.push_back(mask);\n",
    "    }\n",
    "    \n",
    "    input.close();\n",
    "    \n",
    "    // Run inference\n",
    "    InferenceResult result = run_inference(model_path, input_ids, attention_mask);\n",
    "    \n",
    "    // Write results to output file\n",
    "    std::ofstream output(output_file);\n",
    "    if (!output.is_open()) {\n",
    "        std::cerr << \"Could not open output file: \" << output_file << std::endl;\n",
    "        return 1;\n",
    "    }\n",
    "    \n",
    "    output << result.inference_time_ms << std::endl;\n",
    "    \n",
    "    for (const auto& batch_preds : result.predictions) {\n",
    "        for (const auto& pred : batch_preds) {\n",
    "            output << (pred ? 1 : 0) << \" \";\n",
    "        }\n",
    "        output << std::endl;\n",
    "    }\n",
    "    \n",
    "    output.close();\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(\"onnx_inference.cpp\", \"w\") as f:\n",
    "        f.write(cpp_code)\n",
    "    \n",
    "    # Create CMakeLists.txt\n",
    "    cmake_file = \"\"\"\n",
    "cmake_minimum_required(VERSION 3.10)\n",
    "project(ONNXInference)\n",
    "\n",
    "set(CMAKE_CXX_STANDARD 14)\n",
    "set(CMAKE_CXX_STANDARD_REQUIRED ON)\n",
    "\n",
    "# Find ONNX Runtime\n",
    "find_package(onnxruntime REQUIRED)\n",
    "\n",
    "add_executable(onnx_inference_cpp onnx_inference.cpp)\n",
    "target_link_libraries(onnx_inference_cpp onnxruntime)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"CMakeLists.txt\", \"w\") as f:\n",
    "        f.write(cmake_file)\n",
    "    \n",
    "    print(\"C++ code and CMakeLists.txt created.\")\n",
    "    print(\"To compile, you'll need to install ONNX Runtime for C++ and run:\")\n",
    "    print(\"mkdir build && cd build && cmake .. && make\")\n",
    "\n",
    "def prepare_cpp_input_data(tokenizer, texts, filename):\n",
    "    \"\"\"Prepare input data for C++ inference.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].numpy()\n",
    "    attention_mask = inputs['attention_mask'].numpy()\n",
    "    \n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"{batch_size} {seq_len}\\n\")\n",
    "        \n",
    "        # Write input_ids\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                f.write(f\"{input_ids[i][j]} \")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Write attention_mask\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                f.write(f\"{attention_mask[i][j]} \")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    return batch_size, seq_len\n",
    "\n",
    "def run_cpp_inference(model_path, input_file, output_file):\n",
    "    \"\"\"Run inference using the compiled C++ executable.\"\"\"\n",
    "    cmd = [f\"./{CPP_EXECUTABLE}\", model_path, input_file, output_file]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Read results\n",
    "        with open(output_file, 'r') as f:\n",
    "            inference_time = float(f.readline().strip())\n",
    "            \n",
    "            predictions = []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    preds = [bool(int(p)) for p in line.strip().split()]\n",
    "                    predictions.append(preds)\n",
    "        \n",
    "        return predictions, inference_time\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running C++ inference: {e}\")\n",
    "        print(f\"stdout: {e.stdout}\")\n",
    "        print(f\"stderr: {e.stderr}\")\n",
    "        return None, None\n",
    "\n",
    "# System info utilities\n",
    "def get_system_info():\n",
    "    \"\"\"Get system information for reporting.\"\"\"\n",
    "    cpu_info = cpuinfo.get_cpu_info()\n",
    "    return {\n",
    "        \"os\": platform.platform(),\n",
    "        \"cpu\": cpu_info.get('brand_raw', 'Unknown CPU'),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"onnxruntime_version\": ort.__version__,\n",
    "        \"ram\": f\"{psutil.virtual_memory().total / (1024**3):.2f} GB\"\n",
    "    }\n",
    "\n",
    "# Benchmarking functions\n",
    "def benchmark_model(model_name, model_type, texts, \n",
    "                   pt_quantized=False, onnx_quantized=False, \n",
    "                   batch_sizes=[1, 4, 16], cpp_enabled=True):\n",
    "    \"\"\"Benchmark a model using both PyTorch and ONNX Runtime.\"\"\"\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"quantized\": pt_quantized or onnx_quantized,\n",
    "        \"batch_sizes\": batch_sizes,\n",
    "        \"pytorch\": [],\n",
    "        \"onnx\": [],\n",
    "        \"cpp\": []\n",
    "    }\n",
    "    \n",
    "    # Get the right paths\n",
    "    pt_suffix = \"-Quantized\" if pt_quantized else \"\"\n",
    "    onnx_suffix = \"-Quantized\" if onnx_quantized else \"\"\n",
    "    \n",
    "    pt_path = os.path.join(PT_DIR, f\"{model_name}{pt_suffix}_model.pt\")\n",
    "    onnx_path = os.path.join(ONNX_DIR, f\"{model_name}{onnx_suffix}_model.onnx\")\n",
    "    \n",
    "    # Get tokenizer\n",
    "    tokenizer = get_tokenizer(model_type)\n",
    "    \n",
    "    # PyTorch inference\n",
    "    if os.path.exists(pt_path):\n",
    "        model = load_pytorch_model(pt_path, model_type)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Benchmarking {model_name}{pt_suffix} (PyTorch) with batch size {batch_size}...\")\n",
    "            \n",
    "            # Track inference time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Track emissions\n",
    "            tracker = EmissionsTracker(\n",
    "                project_name=f\"{model_name}-PT-bs{batch_size}\",\n",
    "                output_dir=RESULTS_DIR,\n",
    "                measure_power_secs=1,\n",
    "                save_to_file=False\n",
    "            )\n",
    "            tracker.start()\n",
    "            \n",
    "            # Run inference multiple times for better measurement\n",
    "            for _ in tqdm(range(10)):\n",
    "                _ = pytorch_inference(model, tokenizer, texts, batch_size=batch_size)\n",
    "            \n",
    "            emissions = tracker.stop()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Get memory usage\n",
    "            memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)  # in MB\n",
    "            \n",
    "            results[\"pytorch\"].append({\n",
    "                \"batch_size\": batch_size,\n",
    "                \"inference_time\": (end_time - start_time) / 10,  # average time\n",
    "                \"emissions\": emissions,\n",
    "                \"memory_usage_mb\": memory_usage\n",
    "            })\n",
    "    else:\n",
    "        print(f\"Warning: PyTorch model file {pt_path} not found.\")\n",
    "    \n",
    "    # ONNX inference\n",
    "    if os.path.exists(onnx_path):\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Benchmarking {model_name}{onnx_suffix} (ONNX) with batch size {batch_size}...\")\n",
    "            \n",
    "            # Track inference time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Track emissions\n",
    "            tracker = EmissionsTracker(\n",
    "                project_name=f\"{model_name}-ONNX-bs{batch_size}\",\n",
    "                output_dir=RESULTS_DIR,\n",
    "                measure_power_secs=1,\n",
    "                save_to_file=False\n",
    "            )\n",
    "            tracker.start()\n",
    "            \n",
    "            # Run inference multiple times for better measurement\n",
    "            for _ in tqdm(range(10)):\n",
    "                _ = onnx_inference(onnx_path, tokenizer, texts, batch_size=batch_size)\n",
    "            \n",
    "            emissions = tracker.stop()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Get memory usage\n",
    "            memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)  # in MB\n",
    "            \n",
    "            results[\"onnx\"].append({\n",
    "                \"batch_size\": batch_size,\n",
    "                \"inference_time\": (end_time - start_time) / 10,  # average time\n",
    "                \"emissions\": emissions,\n",
    "                \"memory_usage_mb\": memory_usage\n",
    "            })\n",
    "    else:\n",
    "        print(f\"Warning: ONNX model file {onnx_path} not found.\")\n",
    "    \n",
    "    # C++ inference (if enabled and compiled)\n",
    "    if cpp_enabled and os.path.exists(CPP_EXECUTABLE) and os.path.exists(onnx_path):\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Benchmarking {model_name}{onnx_suffix} (C++) with batch size {batch_size}...\")\n",
    "            \n",
    "            # Prepare input data for C++\n",
    "            cpp_input_file = os.path.join(RESULTS_DIR, f\"{model_name}_cpp_input.txt\")\n",
    "            cpp_output_file = os.path.join(RESULTS_DIR, f\"{model_name}_cpp_output.txt\")\n",
    "            \n",
    "            # We'll use a subset of texts for C++ inference to match the batch size\n",
    "            subset_texts = texts[:batch_size]\n",
    "            prepare_cpp_input_data(tokenizer, subset_texts, cpp_input_file)\n",
    "            \n",
    "            # Track emissions\n",
    "            tracker = EmissionsTracker(\n",
    "                project_name=f\"{model_name}-CPP-bs{batch_size}\",\n",
    "                output_dir=RESULTS_DIR,\n",
    "                measure_power_secs=1,\n",
    "                save_to_file=False\n",
    "            )\n",
    "            tracker.start()\n",
    "            \n",
    "            # Run inference multiple times for better measurement\n",
    "            all_times = []\n",
    "            for _ in tqdm(range(10)):\n",
    "                _, inference_time = run_cpp_inference(onnx_path, cpp_input_file, cpp_output_file)\n",
    "                if inference_time is not None:\n",
    "                    all_times.append(inference_time)\n",
    "            \n",
    "            emissions = tracker.stop()\n",
    "            \n",
    "            if all_times:\n",
    "                avg_time = sum(all_times) / len(all_times)\n",
    "                \n",
    "                # Get memory usage (this will be for the Python process, not the C++ executable)\n",
    "                memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)  # in MB\n",
    "                \n",
    "                results[\"cpp\"].append({\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"inference_time\": avg_time / 1000,  # convert ms to seconds\n",
    "                    \"emissions\": emissions,\n",
    "                    \"memory_usage_mb\": memory_usage\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Visualization functions\n",
    "def plot_inference_time_comparison(all_results, save_path=None):\n",
    "    \"\"\"Plot inference time comparison across models and frameworks.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Prepare data\n",
    "    model_names = []\n",
    "    frameworks = [\"PyTorch\", \"ONNX\", \"C++\"]\n",
    "    times_by_framework = {fw: [] for fw in frameworks}\n",
    "    batch_size_to_use = 1  # We'll compare using batch size 1\n",
    "    \n",
    "    for result in all_results:\n",
    "        model_name = result[\"model_name\"]\n",
    "        if result[\"quantized\"]:\n",
    "            model_name += \" (Q)\"\n",
    "        model_names.append(model_name)\n",
    "        \n",
    "        # Get inference times for each framework\n",
    "        for fw_key, fw_name in zip([\"pytorch\", \"onnx\", \"cpp\"], frameworks):\n",
    "            fw_results = result[fw_key]\n",
    "            if fw_results:\n",
    "                # Find the batch size we want\n",
    "                batch_result = next((r for r in fw_results if r[\"batch_size\"] == batch_size_to_use), None)\n",
    "                if batch_result:\n",
    "                    times_by_framework[fw_name].append(batch_result[\"inference_time\"])\n",
    "                else:\n",
    "                    times_by_framework[fw_name].append(None)\n",
    "            else:\n",
    "                times_by_framework[fw_name].append(None)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (fw_name, times) in enumerate(times_by_framework.items()):\n",
    "        # Filter out None values\n",
    "        valid_indices = [j for j, t in enumerate(times) if t is not None]\n",
    "        valid_model_names = [model_names[j] for j in valid_indices]\n",
    "        valid_times = [times[j] for j in valid_indices]\n",
    "        \n",
    "        if valid_times:\n",
    "            plt.bar(\n",
    "                x[valid_indices] + (i - 1) * width, \n",
    "                valid_times, \n",
    "                width, \n",
    "                label=fw_name\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Inference Time (seconds)')\n",
    "    plt.title('Inference Time Comparison (Batch Size = 1)')\n",
    "    plt.xticks(x, model_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_emissions_comparison(all_results, save_path=None):\n",
    "    \"\"\"Plot carbon emissions comparison across models and frameworks.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Prepare data\n",
    "    model_names = []\n",
    "    frameworks = [\"PyTorch\", \"ONNX\", \"C++\"]\n",
    "    emissions_by_framework = {fw: [] for fw in frameworks}\n",
    "    batch_size_to_use = 1  # We'll compare using batch size 1\n",
    "    \n",
    "    for result in all_results:\n",
    "        model_name = result[\"model_name\"]\n",
    "        if result[\"quantized\"]:\n",
    "            model_name += \" (Q)\"\n",
    "        model_names.append(model_name)\n",
    "        \n",
    "        # Get emissions for each framework\n",
    "        for fw_key, fw_name in zip([\"pytorch\", \"onnx\", \"cpp\"], frameworks):\n",
    "            fw_results = result[fw_key]\n",
    "            if fw_results:\n",
    "                # Find the batch size we want\n",
    "                batch_result = next((r for r in fw_results if r[\"batch_size\"] == batch_size_to_use), None)\n",
    "                if batch_result:\n",
    "                    emissions_by_framework[fw_name].append(batch_result[\"emissions\"])\n",
    "                else:\n",
    "                    emissions_by_framework[fw_name].append(None)\n",
    "            else:\n",
    "                emissions_by_framework[fw_name].append(None)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (fw_name, emissions) in enumerate(emissions_by_framework.items()):\n",
    "        # Filter out None values\n",
    "        valid_indices = [j for j, e in enumerate(emissions) if e is not None]\n",
    "        valid_model_names = [model_names[j] for j in valid_indices]\n",
    "        valid_emissions = [emissions[j] for j in valid_indices]\n",
    "        \n",
    "        if valid_emissions:\n",
    "            plt.bar(\n",
    "                x[valid_indices] + (i - 1) * width, \n",
    "                valid_emissions, \n",
    "                width, \n",
    "                label=fw_name\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Carbon Emissions (kg CO2eq)')\n",
    "    plt.title('Carbon Emissions Comparison (Batch Size = 1)')\n",
    "    plt.xticks(x, model_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_efficiency_comparison(all_results, save_path=None):\n",
    "    \"\"\"Plot efficiency (inferences per kWh) comparison.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Prepare data\n",
    "    model_names = []\n",
    "    frameworks = [\"PyTorch\", \"ONNX\", \"C++\"]\n",
    "    efficiency_by_framework = {fw: [] for fw in frameworks}\n",
    "    batch_size_to_use = 1  # We'll compare using batch size 1\n",
    "    \n",
    "    for result in all_results:\n",
    "        model_name = result[\"model_name\"]\n",
    "        if result[\"quantized\"]:\n",
    "            model_name += \" (Q)\"\n",
    "        model_names.append(model_name)\n",
    "        \n",
    "        # Calculate efficiency for each framework\n",
    "        for fw_key, fw_name in zip([\"pytorch\", \"onnx\", \"cpp\"], frameworks):\n",
    "            fw_results = result[fw_key]\n",
    "            if fw_results:\n",
    "                # Find the batch size we want\n",
    "                batch_result = next((r for r in fw_results if r[\"batch_size\"] == batch_size_to_use), None)\n",
    "                if batch_result and batch_result[\"emissions\"] > 0:\n",
    "                    # Calculate inferences per kg CO2eq (higher is better)\n",
    "                    # Assuming 10 inferences in our benchmark\n",
    "                    efficiency = 10 / batch_result[\"emissions\"]\n",
    "                    efficiency_by_framework[fw_name].append(efficiency)\n",
    "                else:\n",
    "                    efficiency_by_framework[fw_name].append(None)\n",
    "            else:\n",
    "                efficiency_by_framework[fw_name].append(None)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (fw_name, efficiencies) in enumerate(efficiency_by_framework.items()):\n",
    "        # Filter out None values\n",
    "        valid_indices = [j for j, e in enumerate(efficiencies) if e is not None]\n",
    "        valid_model_names = [model_names[j] for j in valid_indices]\n",
    "        valid_efficiencies = [efficiencies[j] for j in valid_indices]\n",
    "        \n",
    "        if valid_efficiencies:\n",
    "            plt.bar(\n",
    "                x[valid_indices] + (i - 1) * width, \n",
    "                valid_efficiencies, \n",
    "                width, \n",
    "                label=fw_name\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Inferences per kg CO2eq')\n",
    "    plt.title('Energy Efficiency Comparison (Batch Size = 1)')\n",
    "    plt.xticks(x, model_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_batch_size_impact(all_results, metric='inference_time', save_path=None):\n",
    "    \"\"\"Plot the impact of batch size on inference time or emissions.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Set up the plot\n",
    "    if metric == 'inference_time':\n",
    "        plt.ylabel('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
